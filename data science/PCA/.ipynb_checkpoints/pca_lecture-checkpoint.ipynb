{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 10px;\"> \n",
    "# Principal Components Analysis - PCA\n",
    "\n",
    "---\n",
    "DS-SF-31 | Lesson 13 | Instructor: Mario Javier Carrillo\n",
    "\n",
    "$~$\n",
    "\n",
    "_Lesson adopted from [An Introduction to Statistical Learning] [1] and [A Tutorial on Principal Component Analysis] [2]_\n",
    "\n",
    "[1]: http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf    \"An Introduction to Statistical Learning\"\n",
    "[2]: https://arxiv.org/pdf/1404.1100.pdf   \"A Tutorial on Principal Component Analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Components Analysis\n",
    "---\n",
    "* Technique that when faced with a large set of correlated variables, allow us to summarize this set with a smaller number of representative variables that **collectively** explain most of the variability in the original set.\n",
    "* **PCA** is the quintessential \"dimensionality reduction\" algorithm, where _\"dimensionality reduction\"_ = process of combining or collapsing your existing features (columns in $X$) into new features that retain the signal in the original data in fewer variables while ideally reducing noise.\n",
    "* **PCA** is an unsupervised approach, since it involves only a set of features $X_1$, $X_2$, . . . , $X_p$, and no associated response $Y$ \n",
    "* **PCA** produces derived variables to use in supervised methods and is also a tool for data visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Components Analysis \n",
    "---\n",
    "* Sure, but what the heck PCA really means? how does it work? see  the link below (5 min)\n",
    "\n",
    "<a href = http://setosa.io/ev/principal-component-analysis/> PCA Explained Visually </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Components Analysis - the optimization process - part 1\n",
    "---\n",
    "* The idea is that each of the $n$ observations lives in $p$-dimensional space, but not all of these dimensions are equally interesting.\n",
    "* Each of the dimensions found by **PCA** is a linear combination of the $p$ features, so the first principal component of a set of features $X_1$, $X_2$, . . . , $X_p$ is the **normalized** linear combination of the features:\n",
    "$$Z_1 = œÜ_{11}X_1 +œÜ_{21}X_2 +...+œÜ_{p1}X_p$$\n",
    "that have the **largest variance**  where by **normalized** we are referring to:\n",
    "$$\\sum_{j=1}^p œÜ_{j1}^2 = 1$$\n",
    "The elements $œÜ_{11}$,...,$œÜ_{p1}$ are the loadings of the **first** PCA, **AND** together these loadings make up the principal component loading vector whose **SUM** is equal to **1**: $$œÜ_1 = (œÜ_{11}  œÜ_{21} ... œÜ_{p1})^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Components Analysis - the optimization process - part 2\n",
    "---\n",
    "* Assume we have a $n$ √ó $p$ data set $X$, how can we calculate the **first** PCA?\n",
    "    * Remember we are **only** interested in **variance**, therefore assume that each of the variables in $X$ have been centered to have mean zero =>_the column means of X are zero_<= \n",
    "    * We then look for the linear combination of the sample feature values of the form\n",
    "$$z_{i1} = œÜ_{11}x_{i1} +œÜ_{21}x_{i2} +...+œÜ_{p1}x_{ip}$$\n",
    "    * That is subject to a constrain $$\\sum_{j=1}^p œÜ_{j1}^2 = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Components Analysis - the optimization process - part 3\n",
    "---\n",
    "* Therefore the First PCA loading vector solves the following optimization problem:\n",
    "$$ \\text{maximize}_{œÜ_{11},...,œÜ_{p1}} \\left(\\frac1n\\sum_{i=1}^n \\left(\\sum_{j=1}^p œÜ_{j1}x_{ij} \\right)^2\\right) sub. to \\sum_{j=1}^p œÜ_{j1}^2 = 1$$\n",
    "* after some math:\n",
    "$$\\frac1n \\sum_{i=1}^n z_{i1}^2$$\n",
    "* and since $$\\frac1n \\sum_{i=1}^n x_{ij} = 0$$\n",
    "the average of the $z_{11}$,...,$z_{n1}$ will be zero as well, thus our **maximization** is the **sample variance of the $n$ values of $z_{i1}$ \n",
    "where $z_{11}$,...,$z_{n1}$ are the scores of the first PCA** => solved by eigen decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Components Analysis - the optimization process - part 4\n",
    "---\n",
    "\n",
    "* Once the **First PCA** $Z_1$ has been determine, we can calculate the **Second PCA,** where the second PCA is the linear combination of $X_1$,...$X_p$ that has maximal variance out of all linear combinations that are **uncorrelated with $Z_1$**.\n",
    "* now the second PCA scores $Z_{12}$, $Z_{22}$,...,$Z_{n2}$ take the following form:\n",
    "$$Z_{i2} = œÜ_{12}X_{i1} +œÜ_{22}X_{i2} +...+œÜ_{p2}X_{ip}$$\n",
    "where $œÜ_2$ is the second PCA loading vector with elements $œÜ_{12}$,  $œÜ_{22}$...,$œÜ_{p2}$\n",
    "* Note: constraining $Z_2$ to be uncorrelated with $Z_1$, is equivalent to constraining the direction of $œÜ_2$ to be perpendicular = orthogonal (see graph on next slide) to the direction of $œÜ_1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Principal Components Analysis - PC 1 and PC 2\n",
    "---\n",
    "\n",
    "![](https://snag.gy/ECsJye.jpg)\n",
    "*Image from Introduction to Statistical Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Components Analysis - the process\n",
    "---\n",
    "* Linearly transform an $ùëÅ$√ó$ùëë$ matrix $ùëã$ into an $ùëÅ$√ó$ùëö$ matrix $ùëå$\n",
    "    * Centralized the data (subtract the mean). \n",
    "    * Calculate the $ùëë$√ó$ùëë$ covariance matrix: $$ùê∂ = \\frac1{N-1} X^T X$$\n",
    "        * $C_{ij}$ =  $\\frac1{N-1}$ $\\sum_{q=1}^N$ $X_{q,i}$ $X_{q,i}$ \n",
    "        * $C_{i,i}$ (diagonal) is the variance of variable $i$\n",
    "        * $C_{i,j}$ (off-diagonal) is the covariance between variables $i$ and $j$\n",
    "    * Calculate the **eigenvectors** of the covariance matrix\n",
    "         * An **eigenvector** specifies a direction through the original coordinate space. \n",
    "    * Select $m$ **eigenvectors** that correspond to the **largest $m$ eigenvalues** to be the new basis.\n",
    "         * The eigenvector with the highest correspoding **eigenvalue** is the first principal component.\n",
    "         * **Eigenvalues** indicate the amount of variance in the direction of it's corresponding eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Principal Components Analysis - eigenvectors\n",
    "---\n",
    "* If $A$ is a **square matrix**, a non-zero vector **$v$** is an **eigenvector** of $A$ if there is a scalar $Œª$ **(eigenvalue)** such that $$Av = Œªv$$\n",
    "\n",
    "* For example:\n",
    "$$ Av = \n",
    "\\left(\\begin{array}{cc} \n",
    "2 & 3\\\\\n",
    "2 & 1\n",
    "\\end{array}\\right) *\n",
    "\\left(\\begin{array}{cc} \n",
    "3 \\\\ \n",
    "2 \n",
    "\\end{array}\\right) = \n",
    "\\left(\\begin{array}{cc} \n",
    "12 \\\\\n",
    "8\n",
    "\\end{array}\\right) = \n",
    "4\n",
    "\\left(\\begin{array}{cc} \n",
    "3 \\\\\n",
    "2\n",
    "\\end{array}\\right)\n",
    "= Œªv\n",
    "$$ \n",
    "$~$\n",
    "* If you think of the squared matrix $A$ as a transformation matrix, then multiply it with the **eigenvector do not change its direction.**\n",
    "\n",
    "<a href = http://setosa.io/ev/eigenvectors-and-eigenvalues/> Please see Eigenvectors and Eigenvalues Visually </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Components Analysis - in sum\n",
    "---\n",
    "\n",
    "What is a principal component? **Principal components are the vectors that define the new coordinate system for your data.** Transforming your original data columns onto the principal component axes constructs new variables that are optimized to explain as much variance as possible and to be independent (uncorrelated).\n",
    "\n",
    "Creating these variables is a well-defined mathematical process, but in essence **each component is created as a weighted sum of your original columns, such that all components are orthogonal (perpendicular) to each other**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Components Analysis - in sum\n",
    "---\n",
    "![](https://snag.gy/0Hur9o.jpg)\n",
    "*Image from http://setosa.io/ev/principal-component-analysis/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Components Analysis - Why would we want to do PCA?\n",
    "---\n",
    "* We can reduce the number of dimensions (remove bottom number of components) and lose the least possible amount of variance information in our data.\n",
    "* Since we are assuming our variables are interrelated (at least in the sense that they together explain a dependent variable), the information of interest should exist along directions with largest variance.\n",
    "* The directions of largest variance should have the highest Signal to Noise ratio.\n",
    "* Correlated predictor variables (also referred to as \"redundancy\" of information) are combined into independent variables. Our predictors from PCA are guaranteed to be independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Components Analysis - Want to explore more on PCA ?\n",
    "---\n",
    "[Performing PCA by Sebastian Raschka](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)\n",
    "\n",
    "[PCA 4 dummies](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)\n",
    "\n",
    "[Stackoverflow making sense of PCA](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n",
    "\n",
    "[PCA and spectral theorem](http://stats.stackexchange.com/questions/217995/what-is-an-intuitive-explanation-for-how-pca-turns-from-a-geometric-problem-wit)\n",
    "\n",
    "[PCA math and examples](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
