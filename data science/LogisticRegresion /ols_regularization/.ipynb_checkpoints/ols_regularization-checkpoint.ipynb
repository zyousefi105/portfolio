{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 10px;\"> \n",
    "# OLS and Regularization\n",
    "\n",
    "---\n",
    "DS-SF-31| Lesson 8.1 | Mario Javier Carrillo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Parametric Methods - OLS\n",
    "In ML **Causality is de-emphasized.** Methods are largely about **_prediction and fit_.** That is important in allowing validation of methods through out-of-sample crossvalidation.\n",
    "1. Parametric methods involve a two-step model-based approach:\n",
    "\n",
    "    1.1 Make an assumption about the functional form, or shape, of $f$ (OLS => i.e. $f$ is linear in $X$)\n",
    "$$ f(X)=β_0 +β_1X_1 +β_2X_2 +...+β_pX_p.$$\n",
    "    \n",
    "    1.2 Once a model is selected we need a procedure that uses the training data to _fit_ or train the model In the case of our linear model we need to estimate parameters $β_0$+$β_1$+$β_2$+...+$β_p$. This means that we need to find values of these parameters so that\n",
    "    \n",
    "$$Y ≈β_0 +β_1X_1 +β_2X_2 +...+β_pX_p$$\n",
    "\n",
    "where “≈” as “is approximately modeled as”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://snag.gy/iVykz0.jpg)\n",
    "*Image from Python Machine Learning - Sebastian Raschka*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - OLS - Assesing the accuracy of the model\n",
    "* Let $\\hat Y_i$ = $\\hat β_0$ + $\\hat β_1x_i$ be the prediction for $Y$ based on the $i$th value of $X$. \n",
    "* Then $e_i$ = $y_i$ − $\\hat y_i$ represents the $i$th residual => this is the difference between the $i$th observed response value and the $i$th response value that is predicted by a  linear model\n",
    "* We define the residual sum of squares (RSS) as\n",
    "$$R S S = e^2_1 + e^2_2 + · · · + e^2_n$$\n",
    "\n",
    "* Residual Standard Error (RSE), associated with each observation is an error term $ε$, and is because the presence of the error terms that we are unable to predict $Y$ from $X$\n",
    "* RSE is an estimate of the standard deviation of $ε$, thus RSE is\n",
    "$$RSE = \\sqrt {\\frac{1} {n-2} RSS}$$\n",
    "\n",
    "Where \n",
    "$$R S S  = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2  $$\n",
    "\n",
    "\n",
    "* Mean Square Error (MSE) => average of the sum of the square errors\n",
    "$$ MSE =\\frac 1n\\sum_{i=1}^n (y_i - \\hat{y}_i)^2  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - the least squares loss function\n",
    "\n",
    "You've become familiar at this point with the least squares loss function. Vanilla regression minimizes the residual sum of squares (RSS) to fit the data:\n",
    "\n",
    "$$ \\text{minimize}\\; RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 $$\n",
    "\n",
    "Where our model predictions for $y$ are based on the sum of the $beta_0$ intercept and the products of $\\beta_i$ with $x_i$.\n",
    "\n",
    "[Loss Function] [1]\n",
    "[1]: https://davidrosenberg.github.io/ml2015/docs/3a.loss-functions.pdf \"Loss Function\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Leaning - Regularization\n",
    "* Rather than simply choosing the best fit, there is some penalty to avoid over-fitting.\n",
    "    * Model suffers from **overfitting** => high variance (too many parameters that create a complex model)\n",
    "    * Model suffers from **underfitting** => high bias (model is not complex enough to capture patterns on our train data, suffers from  low perfomance\n",
    "    * Here lies the bias-variance tradeoff, but we can tune the complexity of the model via **regularization**\n",
    "    * **Regularization** handles  **multicollinearity**, filters out noise from data, eventually will prevent overfitting \n",
    "    * **Regularization** the idea is to introduce additional information (bias) to penalize extreme parameter weights\n",
    "\n",
    "* Two issues: choosing the form of the regularization, and choosing the amount of regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - Ridge regression $L_2$\n",
    "\n",
    "The Ridge regression adds an additional thing to the loss function: the sum of the squared (non-intercept!) $\\beta$ values:\n",
    "$$ \\text{minimize}\\; RSS+Ridge = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "What are these new components?\n",
    "\n",
    "$\\beta_j^2$ is the squared coefficient for variable $x_j$.\n",
    "\n",
    "$\\sum_{j=1}^n \\beta_ij^2$ is the sum of these squared coefficients for every variable we have in our model. This does **not** include the intercept $\\beta_0$.\n",
    "\n",
    "$\\lambda_2$ is a constant for the _strength_ of the regularization parameter. The higher this value, the greater the impact of this new component in the loss function. If this were zero, then we would revert back to just the least squares loss function. If this were, say, a billion, then the residual sum of squares component would have a much smaller effect on the loss/cost than the regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - Lasso regression $L_1$\n",
    "\n",
    "The Lasso regression takes a different approach. Instead of adding the sum of _squared_ $\\beta$ coefficients to the RSS, it adds the sum of the _absolute value_ of the $\\beta$ coefficients:\n",
    "\n",
    "$$ \\text{minimize}\\; RSS + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "$|\\beta_j|$ is the absolute value of the $\\beta$ coefficient for variable $x_j$\n",
    "\n",
    "$\\lambda_1$ is again the strength of the regularization penalty component in the loss function. In lasso the lambda is denoted with a 1, in ridge the lambda is denoted with a 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - Elastic Net\n",
    "\n",
    "Elastic Net is a combination of both the Lasso and the Ridge regularizations. It adds both penalties to the loss function:\n",
    "\n",
    " $$ \\text{minimize}\\; RSS + Ridge + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j| + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "In the elastic net, the effect of the Ridge vs. the Lasso is balanced by the two lambda parameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - so when do you use each? what is the effect of regularization?\n",
    "\n",
    "When you have **multicollinearity** in the data.\n",
    "\n",
    "The Lasso and Elastic Net are also very useful for when you have redundant or unimportant variables. If you have 1000 variables in a dataset the Lasso can perform \"feature selection\" automatically for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning - normalize predictors \n",
    "With the Lasso and Ridge it is neccessary to **standarized** the predictor columns before constructing models, even the dummy coded categorical variables. \n",
    "\n",
    "\n",
    "#### Why is normalization of predictors required?\n",
    "\n",
    "Recall the equations for the Ridge and Lasso penalties:\n",
    "\n",
    "$$ \\text{Ridge penalty}\\; = \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "$$ \\text{Lasso penalty}\\; = \\lambda_2\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "**How are the $\\beta$ coefficients affected by the mean and variance of your variables?**\n",
    "\n",
    "If the mean and variance of your $x$ predictors are different, their respective $\\beta$ coefficients _scale with the mean and variance of the predictors **regardless of their explanatory power.**_\n",
    "\n",
    "This means that if one of your $x$ variables, for example the price of a home, will have a much smaller $\\beta$ value than say the number of bedrooms in a house – just because the scale of the two variables are so different.\n",
    "\n",
    "The Ridge and Lasso penalties are **agnostic** to the **mean** and **variance** of your predictors. All they \"care about\" are the values of the coefficients. If one of your coefficients is much larger than any of the others, it will dominate the effect of the penalty on your minimization!\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
